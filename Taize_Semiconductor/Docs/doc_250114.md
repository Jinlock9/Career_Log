# Document 25.01.14

## Manual

### Setup Docker
```sh
# Setup
docker run --name heigui-dev --net host -v /wkspc/heigui/:/wkspc/heigui --security-opt seccomp=unconfined --privileged=true -it ubuntu

# Refresh
newgrp docker

# Check Running Process
docker ps -a
```

### Connect to Docker Container
- `Ctrl + Shift + P`
- `Dev-Containers: Attach to Running Container...`

### LLVM BUILD
```sh
# Preparation
apt-get update
apt update

# Install Libraries
apt install python3 python3-pip
apt install lld
pip install cmake
pip install Ninja

# Compile
# in LLVM repo
mkdir build && cd build
cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug -DLLVM_USE_LINKER=lld -DLLVM_ENABLE_PROJECTS="clang;lld" -DLLVM_TARGETS_TO_BUILD="X86" -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD="DLC" ../llvm
ninja
```

## Material

### Very Long Instruction Word (VLIW) Architecture

Instruction Level Parallelism, such as *pipelining*, *superscalar processor*, and *out-of-order execution* add extra hardware complexity. **VLIW Architecture** deals with it by depending on the **compiler**. The programs decide the parallel flow of the instructions and to resolve conflicts. This increases compiler complexity but decreases hardware complexity by a lot. [1]

#### Features [1]
- The processors in this architecture have multiple functional units, fetch from the Instruction cache that have the *Very Long Instruction Word*.
- Multiple indepedent operations are grouped together in a single VLIW Instructions. They are initialized in the same clock cycle.
- Each operation is assigned an independent functional unit.
- All the functional units share a common register file.
- Instruction words are typically of the length of 64-1024 bits depending on the number of execution unit and the code length required to control each unit.
- Instruction scheduling and parallel dispatch of the word is done statically by the compiler.
- The compiler checks for dependencies before scheduling parallel execution of the instruction.

---

### Half-precision floating-point format

In computing, **half precision** (sometimes called **FP16** or **float16**) is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory. It is intended for storage of floating-point values in applications where higher precision is not essential. [2]

Depending on the computer, half-precision can be over an order of magnitude faster than double precision, e.g. 550 PFLOPS for half-precision vs 37 PFLOPS for double precision on one cloud provider. [2]

#### IEEE 754 half-precision binary floating-point format: binary16 [2]
The IEEE 754 standard specifies a **binary16** as having following format:
- Sign bit: 1 bit
- Exponent width: 5 bits
- Significand precision: 11 bits (10 explicitly stored)
The format is laid out as follows: `15(sign), 14-10(exponent), 9-0(fraction)`
The format is assumed to have an implicit lead bit with value 1 unless the exponent field is stored with all zeros. Thus, only 10 bits of the significand appear in the memory format but the total precision is 11 bits.

---

### Stride of an array

In computer programming, the stride of an array (also referred to as increment, pitch or step size) is the number of locations in memory between beginnings of successive array elements, measured in bytes or in units of the size of the array's elements. The stride cannot be smaller than the element size but can be larger, indicating extra space between elements.

An array with stride of exactly the same size as the size of each of its elements is contiguous in memory. Such arrays are sometimes said to have unit stride. Unit stride arrays are sometimes more efficient than non-unit stride arrays, but non-unit stride arrays can be more efficient for 2D or multi-dimensional arrays, depending on the effects of caching and the access patterns used. This can be attributed to the principle of locality, specifically spatial locality. [3]

---

### Tensor Processing Unit (TPU)

A tensor processing unit (TPU) is an application-specific integrated circuit (ASIC) specifically designed to accelerate high-volume mathematical and logical processing tasks typically involved with machine learning (ML) workloads. [4]

#### How do TPUs work?

TPUs provide a limited number of features and functionalities that are directly useful to ML and artificial intelligence (AI) tasks but are not necessarily useful for everyday general computing. ML models and the AI platforms that use them, such as deep learning and neural networks, require extensive mathematical processing. While it's possible to execute these tasks in ordinary central processing units (CPUs) or more advanced graphics processing units (GPUs), neither is optimized for such tasks.

Just as GPUs arose to speed the math processing required for gaming and data visualization, TPUs now accelerate the mathematical tasks used for neural networks and other ML models. This includes multiplication and accumulation, or addition, operations.

A TPU employs one or more extensive arrays of multiply-and-accumulate arithmetic logic units (ALUs) configured as a matrix. This matrix processing solves extensive mathematical tasks much faster and with far lower power consumption than more traditional processor types. In short, a TPU takes input data, breaks down the data into multiple tasks called vectors, performs multiplication and addition on each vector simultaneously and in parallel, and then delivers the output to the ML model.

Recent TPU designs automatically adjust performance depending on the supported application type. TPUs also handle low-level dataflow graphs and tackle sophisticated graph calculations that tax traditional CPUs and GPUs. TPUs support 16-bit floating point operations and use high-bandwidth memory; late-model TPUv5p chips list a memory bandwidth of 2,765 GB/s. [4]

---

### LLVM's DAG Selection (by ChatGPT 4o mini)

LLVM's **DAG (Directed Acyclic Graph) Selection** is a key component of the LLVM code generation phase. It is part of the **SelectionDAG framework**, which is used to lower LLVM IR (Intermediate Representation) to machine-specific instructions. Below is a detailed explanation:

#### Key Concepts

1. **SelectionDAG**: 
   - A data structure used in LLVM for representing code during the instruction selection phase.
   - It models operations and values as nodes in a directed acyclic graph (DAG). Each node represents an operation (e.g., addition) or a value (e.g., a constant or a register).
   - Edges represent data dependencies between nodes.

2. **Instruction Selection**:
   - The process of translating LLVM IR into a sequence of machine instructions specific to a target architecture.
   - Involves pattern matching between the DAG nodes and patterns defined in the target's **TableGen** files.

#### Steps in DAG Selection

1. **DAG Construction**:
   - LLVM IR is lowered into a SelectionDAG. During this step:
     - Abstract LLVM operations are converted into target-independent nodes in the DAG.
     - Nodes may represent arithmetic operations, memory accesses, or control flow.

2. **DAG Optimization**:
   - Target-independent optimizations are performed on the DAG.
   - Simplifies the DAG to reduce redundancy or optimize for performance.
   - Example optimizations include constant folding and dead code elimination.

3. **Instruction Selection (Pattern Matching)**:
   - The target-specific instruction selection phase begins.
   - Uses **TableGen**-generated rules to match patterns in the DAG to machine instructions.
     - **TableGen** defines patterns in the target's **.td** files.
   - Complex patterns may be matched using custom logic if predefined patterns are insufficient.

4. **DAG to Machine Instruction Conversion**:
   - The matched nodes in the DAG are translated into machine-specific instructions.
   - Remaining target-independent DAG nodes are expanded or lowered into a sequence of target-specific instructions.

5. **Post-Instruction Selection**:
   - The generated instructions are further optimized for the target architecture.
   - Scheduling and register allocation are performed after DAG selection.

#### Advantages of DAG Selection

1. **Abstraction**: 
   - Separates target-independent and target-specific logic, making it easier to support new architectures.

2. **Optimization Opportunities**:
   - The DAG representation allows for various optimizations before instruction selection.

3. **Pattern Matching Efficiency**:
   - The TableGen-based approach enables efficient and maintainable pattern matching.

#### TableGen and DAG Patterns
- **TableGen**:
  - A domain-specific language in LLVM used to define target-specific instructions and patterns.
  - Defines mappings between DAG node patterns and machine instructions.

- **Example Pattern**:
  ```llvm
  def : Pat<(add GPR:$a, GPR:$b), ADDrr:$a, $b>;
  ```
  This rule specifies that an `add` operation on two general-purpose registers (`GPR`) maps to the `ADDrr` machine instruction.

#### Summary of Steps
1. LLVM IR is lowered into a SelectionDAG.
2. Target-independent optimizations are applied to the DAG.
3. Pattern matching is used to map DAG nodes to machine-specific instructions.
4. DAG nodes are translated into machine instructions.
5. The resulting instructions undergo further optimization and processing.

---

### Reference
[1] https://www.geeksforgeeks.org/very-long-instruction-word-vliw-architecture/  
[2] https://en.wikipedia.org/wiki/Half-precision_floating-point_format  
[3] https://en.wikipedia.org/wiki/Stride_of_an_array  
[4] https://www.techtarget.com/whatis/definition/tensor-processing-unit-TPU  