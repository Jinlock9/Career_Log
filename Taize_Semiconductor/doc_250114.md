# Document 25.01.14

## Manual

### Setup Docker
```sh
# Setup
docker run --name heigui-dev --net host -v /wkspc/heigui/:/wkspc/heigui --security-opt seccomp=unconfined --privileged=true -it ubuntu

# Refresh
newgrp docker

# Check Running Process
docker ps -a
```

### Connect to Docker Container
- `Ctrl + Shift + P`
- `Dev-Containers: Attach to Running Container...`

### LLVM BUILD
```sh
# Preparation
apt-get update
apt update

# Install Libraries
apt install python3 python3-pip
apt install lld
pip install cmake
pip install Ninja

# Compile
# in LLVM repo
mkdir build && cd build
cmake -G Ninja -DCMAKE_BUILD_TYPE=Debug -DLLVM_USE_LINKER=lld -DLLVM_ENABLE_PROJECTS="clang;lld" -DLLVM_TARGETS_TO_BUILD="X86" -DLLVM_EXPERIMENTAL_TARGETS_TO_BUILD="DLC" ../llvm
ninja
```

## Material

### Very Long Instruction Word (VLIW) Architecture

Instruction Level Parallelism, such as *pipelining*, *superscalar processor*, and *out-of-order execution* add extra hardware complexity. **VLIW Architecture** deals with it by depending on the **compiler**. The programs decide the parallel flow of the instructions and to resolve conflicts. This increases compiler complexity but decreases hardware complexity by a lot. [1]

#### Features [1]
- The processors in this architecture have multiple functional units, fetch from the Instruction cache that have the *Very Long Instruction Word*.
- Multiple indepedent operations are grouped together in a single VLIW Instructions. They are initialized in the same clock cycle.
- Each operation is assigned an independent functional unit.
- All the functional units share a common register file.
- Instruction words are typically of the length of 64-1024 bits depending on the number of execution unit and the code length required to control each unit.
- Instruction scheduling and parallel dispatch of the word is done statically by the compiler.
- The compiler checks for dependencies before scheduling parallel execution of the instruction.

---

### Half-precision floating-point format

In computing, **half precision** (sometimes called **FP16** or **float16**) is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory. It is intended for storage of floating-point values in applications where higher precision is not essential. [2]

Depending on the computer, half-precision can be over an order of magnitude faster than double precision, e.g. 550 PFLOPS for half-precision vs 37 PFLOPS for double precision on one cloud provider. [2]

#### IEEE 754 half-precision binary floating-point format: binary16 [2]
The IEEE 754 standard specifies a **binary16** as having following format:
- Sign bit: 1 bit
- Exponent width: 5 bits
- Significand precision: 11 bits (10 explicitly stored)
The format is laid out as follows: `15(sign), 14-10(exponent), 9-0(fraction)`
The format is assumed to have an implicit lead bit with value 1 unless the exponent field is stored with all zeros. Thus, only 10 bits of the significand appear in the memory format but the total precision is 11 bits.

---

### Stride of an array

In computer programming, the stride of an array (also referred to as increment, pitch or step size) is the number of locations in memory between beginnings of successive array elements, measured in bytes or in units of the size of the array's elements. The stride cannot be smaller than the element size but can be larger, indicating extra space between elements.

An array with stride of exactly the same size as the size of each of its elements is contiguous in memory. Such arrays are sometimes said to have unit stride. Unit stride arrays are sometimes more efficient than non-unit stride arrays, but non-unit stride arrays can be more efficient for 2D or multi-dimensional arrays, depending on the effects of caching and the access patterns used. This can be attributed to the principle of locality, specifically spatial locality. [3]

---

### Tensor Processing Unit (TPU)

A tensor processing unit (TPU) is an application-specific integrated circuit (ASIC) specifically designed to accelerate high-volume mathematical and logical processing tasks typically involved with machine learning (ML) workloads. [4]

#### How do TPUs work?

TPUs provide a limited number of features and functionalities that are directly useful to ML and artificial intelligence (AI) tasks but are not necessarily useful for everyday general computing. ML models and the AI platforms that use them, such as deep learning and neural networks, require extensive mathematical processing. While it's possible to execute these tasks in ordinary central processing units (CPUs) or more advanced graphics processing units (GPUs), neither is optimized for such tasks.

Just as GPUs arose to speed the math processing required for gaming and data visualization, TPUs now accelerate the mathematical tasks used for neural networks and other ML models. This includes multiplication and accumulation, or addition, operations.

A TPU employs one or more extensive arrays of multiply-and-accumulate arithmetic logic units (ALUs) configured as a matrix. This matrix processing solves extensive mathematical tasks much faster and with far lower power consumption than more traditional processor types. In short, a TPU takes input data, breaks down the data into multiple tasks called vectors, performs multiplication and addition on each vector simultaneously and in parallel, and then delivers the output to the ML model.

Recent TPU designs automatically adjust performance depending on the supported application type. TPUs also handle low-level dataflow graphs and tackle sophisticated graph calculations that tax traditional CPUs and GPUs. TPUs support 16-bit floating point operations and use high-bandwidth memory; late-model TPUv5p chips list a memory bandwidth of 2,765 GB/s. [4]

---

### Reference
[1] https://www.geeksforgeeks.org/very-long-instruction-word-vliw-architecture/
[2] https://en.wikipedia.org/wiki/Half-precision_floating-point_format
[3] https://en.wikipedia.org/wiki/Stride_of_an_array
[4] https://www.techtarget.com/whatis/definition/tensor-processing-unit-TPU